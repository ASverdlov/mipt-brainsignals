\documentclass[12pt,twoside]{article}
\usepackage{jmlda}

\newcommand{\bff}{\mathbf{f}}
\newcommand{\ba}{\mathbf{a}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\btheta}{\boldsymbol{\theta}}
\newcommand{\bbeta}{\boldsymbol{\beta}}
\newcommand{\bY}{\mathbf{Y}}
\newcommand{\bX}{\mathbf{X}}
\newcommand{\bTheta}{\boldsymbol{\Theta}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\bu}{\mathbf{u}}
\newcommand{\bt}{\mathbf{t}}
\newcommand{\bp}{\mathbf{p}}
\newcommand{\bq}{\mathbf{q}}
\newcommand{\bc}{\mathbf{c}}
\newcommand{\bbb}{\mathbf{b}}
\newcommand{\bP}{\mathbf{P}}
\newcommand{\bT}{\mathbf{T}}
\newcommand{\bQ}{\mathbf{Q}}
\newcommand{\bC}{\mathbf{C}}
\newcommand{\bE}{\mathbf{E}}
\newcommand{\bF}{\mathbf{F}}
\newcommand{\bU}{\mathbf{U}}
\newcommand{\bW}{\mathbf{W}}
\newcommand{\bI}{\mathbf{I}}
\newcommand{\Sim}{\myop{Sim}}
\newcommand{\Rel}{\myop{Rel}}
\newcommand{\var}{\textrm{var}}
\newcommand{\cJ}{\mathcal{J}}
\newcommand{\cA}{\mathcal{A}}
\newcommand{\cQ}{\mathcal{Q}}
\newcommand{\xfeat}{\mathbf{\chi}}
\newcommand{\yfeat}{y}
\newcommand{\BB}{\mathbb{B}}

%\NOREVIEWERNOTES
\title
    {Прогнозирование намерений по cигналам ECoG}
\author
    {Калиниченко~О.\,И., Ремизова~А.\,С.} % основной список авторов, выводимый в оглавление
\thanks
{	Научный руководитель:  Стрижов~В.\,В. 
	Задачу поставил:  Стрижов~В.\,В.
	Консультант:  Исаченко~Р.\,В.}
% \email{author@site.ru}
\organization
{$^1$ Московский физико-технический институт}
\abstract
{Работа посвящена построению системы тестирования прогностических моделей для различных критериев качества.
	Рассматривается случай коррелированных входных и выходных сигналов электрокортикограммы и фазовых траекторий движения конечностей высокой размерности.
	Ставится задача предсказания намерений по сигналам головного мозга.
	Входные данные -- сигналы электрокортикограммы (ECoG).
	Для выявления и устранения скрытых зависимостей используются методы снижения размерности пространства и отбора признаков.
	Предложенная система тестирования оценивает качество и устойчивость прогноза моделей.
	Вычислительный эксперимент проводится на данных ECoG проекта NeuroTycho.
	
	\bigskip
	\textbf{Ключевые слова}: \emph {декодирование временных рядов, отбор признаков, PLS, QPFS, генетический алгоритм, BCI, электрокортикограмма, траектории движения конечностей}.}

\begin{document}
\maketitle



\section{Введение}
Работа посвящена исследованию методов моделирования нейросетевого интерфейса (BCI) \cite{Motrenko17ECoG}.
Входные данные -- сигналы мозга, полученные с помощью электрокортикографии (ECoG) и электроэнцефалографии (EEG). ECoG-сигналы имеют более высокое разрешение и большую амплитуду, однако для их получения требуется непосредственное подсоединение электродов к коре головного мозга. Одной из задач при построении систем BCI является предсказание намерений.

Предлагается декодировать исходные сигналы и спрогнозировать траекторию движения верхних конечностей. Исходное пространство имеет избыточно высокую размерность. Линейная зависимость между признаками приводит к мультиколлинеарности. Для устранения мультиколлинеарности предлагается применить методы понижения размерности и отбора признаков.

Признаковое описание многомерного временного ряда существует в пространствах независимых и зависимых переменных. Для учета существующих закономерностей в исходном и выходном пространстве используется скрытое пространство латентных переменных.  В скрытом пространстве происходит согласование между образами исходных пространств.

В эксперименте рассматриваются следующие модели: линейная регрессия, метод частных наименьших квадратов (PLS) \cite{Isachenko17PLS}, а также методы отбора признаков с помощью квадратичного программирования (QPFS) \cite{Katrutsa17QPFS} и генетического алгоритма.
% Нелинейные модели -- нейросети.

Предлагается система тестирования прогностических моделей с оценкой качества и анализом ошибки. Подобный инструмент может применяться не только в задаче анализа сигналов мозга, но и во многих других задачах, связанных с прогнозированием многомерных временных рядов.

\section{Постановка задачи}
\paragraph{Постановка задачи предсказания}

Задана выборка $\mathfrak{D}= \left( \bX, \bY \right)$, где $\bX \in \RR^{m \times n}$ --- матрица объектов, $\bX = \left[ \xfeat_1 \dots \xfeat_n \right]$, где $\xfeat_j \in \mathbb{R}^{m}$ --- вектор значений $j$-го признака на элементах выборки; $\bY \in \RR^{m \times r}$ --- матрица ответов, $\bY = \left[ \yfeat_1 \dots \yfeat_r \right]$. Имеется линейная модель $\bff$ с набором параметров $\btheta$ из пространства $\RR^{n \times r}$, предсказывающая
$\by \in \RR^r$ по $\bx \in \RR^n$, следующего вида:
\begin{equation}
\bff(\bx, \btheta) = \underset{1 \times n}{\bx}\T \underset{n \times r}{\btheta}.
\label{eq::linear_model}
\end{equation}


Задана функция ошибки $S$ на выборке $\mathfrak{D}$  и модели $\bff$ с параметрами $\btheta$.
Задачей является поиск наилучших параметров $\btheta^*$, то есть таких, при которых функция ошибки минимальна:
\begin{equation}
\btheta^* = \argmin_{\btheta \in \Theta} S(\btheta| \mathfrak{D}, \bff).
\label{eq::error_function}
\end{equation}

В случае кореллированных данных $\bX$ решение задачи выбора оптимального вектора параметров $\btheta$ нестабильно. Подробное описание устойчивости модели приведено в секции "Анализ ошибки"


За $\bF(\bX, \btheta)$ обозначена матрица $\left[\bff(\bx_1, \btheta), \bff(\bx_2, \btheta), \dots, \bff(\bx_m, \btheta)\right]\T$. Рассматривается квадратичная функция ошибки:
\begin{equation}
{\bigl\| \bF(\bX, \btheta) - \bY \bigr\| }_2^2
\rightarrow \min_{\btheta \in \RR^{n \times r}}
\label{eq::error_linear}
\end{equation}
Если пространство признаков имеет высокую размерность, вероятно, что матрица $\bX$ близка к вырожденной, а потому решение проблемы оптимизации \eqref{eq::error_linear} будет нестабильным. Для решения указанной задачи применяются методы отбора признаков.



\paragraph{Постановка задачи отбора признаков}

Пусть $\cJ = \{ 1, \dots, n \}$ --- множество индексов признаков, а $\cA \subseteq \cJ$ --- его некоторое подмножество. Тогда линейную модель $\bff$ на подмножестве признаков $\cA$ можно определить как:

\begin{equation}
\bff (\bx, \cA, \btheta) = \underset{1 \times |\cA|}{\bx_{\cA}} \underset{|\cA| \times r}{\btheta} \label{eq:fs_model}
\end{equation}

Функция ошибки $S$ теперь зависит от выборки $\mathfrak{D}$, модели $\bff$ с параметрами $\btheta$ и от $\cA$.

Тогда задача предсказания определяется как поиск набора признаков $\cA$ и матрицы параметров $\btheta^* \in \RR^{|\cA| \times r}$ такого, что:
\begin{equation}
\btheta^* = \argmin_{\btheta  \in \RR^{|\cA| \times r}}   S(\theta, \cA | \mathfrak{D}, \bff)   \label{eq::fs_forecast}
\end{equation}


Задача отбора признаков поставлена как:
\begin{equation}
\cA^* = \argmin_{\cA \in cJ} Q(\cA | \bX, \by), \label{eq::fs_problem}
\end{equation}

где $Q : \cA \to \RR$ --- это некоторый критерий качества,который определяет качество выбранного подмножества признаков $\cA \subseteq \cJ$.

Для решения этой задачи не обязательно требуется оценка оптимального вектора параметров $\btheta^*$. Она использует зависимости между векторами $\xfeat_j, j \in \cJ$ и целевыми векторами $y_i, i \in \{1, \dots, r \}$.

Пусть $\ba \in \BB^n = \{0, 1\}^n$ -- вектор индикаторов принадлежности признаков подмножеству $cA$ -- то есть $\ba_j = 1$ тогда и только тогда, когда $j \in \cA$. Тогда задача $\ref{eq::fs_problem}$ может быть записана как:
\begin{equation}
\ba^* = \argmin_{\ba \in \BB^n} Q(\cA | \bX, \by), \label{eq::fs_vec_problem}
\end{equation}
где $Q : \BB^n \to \RR$ -- критерий $Q$, но определенный на $ \BB^n $; связь между вектором $\ba^*$ и множеством $\cA^*$ определяется через:
\begin{equation}
\ba^*_j = 1 \Leftrightarrow j \in \cA^*, j \in \cJ. \label{eq::fs_index_correspondence}
\end{equation}

\section {Модели}

В данной работе используются две линейные модели:

\begin{enumerate}
	\item Линейная регрессия
	\item PLS(Partial Least Squares Regression)
\end{enumerate}
\paragraph{Линейная регрессия}

TODO

\paragraph{PLS}

Метод частных наименьших квадратов PLS рассматривает в качестве признаков линейные комбинации исходных. Предполагается, что существует скрытое пространство латентных переменных малой размерности $l$ ($l < n, r$). Происходит поиск матрицы $\bT \in \RR^{m \times l}$, которая наилучшим образом описывает матрицы $\bX$ и $\bY$.

\begin{align}
\underset{m \times n}{\bX} &= \underset{m \times l}{\bT\T} \cdot
\underset{l \times n}{\bP\T} + \underset{m \times n}{\bE}
\label{eq::PLS_X} \\
\underset{m \times r}{\bY} &= \underset{m \times l}{\bU\T} \cdot
% \underset{l \times l}{\textrm{diag}(\bbeta)}\cdot
\underset{l \times r}{\bQ\T} + \underset{m \times r}{\bF}
\label{eq::PLS_Y}
\end{align}

\section{Методы отбора признаков}

Существуют различные методы отбора признаков. В данной работе используются:
\begin{enumerate}
	\item QPFS
	\item Генетический алгоритм
\end{enumerate}

Приведем краткие описания используемых алгоритмов.

\paragraph{QPFS}
\[
\]
Чтобы вычислить матрицу $\bQ$ и вектор $\bbb$

\begin{align}
\Sim & \label{eq::Sim} \\
\Rel & \label{eq::Rel}
\end{align}

Коэффициент корреляции Пирсона определяется как:

$$\rho_{ij} =\frac{
	\cov(\bx_i, \bx_j)
}{
	\sqrt{\var(\bx_i) \cdot \var(\bx_j)}
} $$


\paragraph{Генетический алгоритм}
\[
\]

Этот алгоритм основан на идеях, заимствованных у природы. Изначально имеется "популяция" прогностических моделей. Каждая модель (в исследовании, которому посвящена данная работа, это линейная модель) характеризуется множеством признаков, которые будут использоваться при ее построении. Изначально множества выбираются случайно. Алгоритм итеративный. На каждом шаге сначала вычисляется метрика качества модели. Выбирается некоторое количество моделей с наилучшей метрикой. Далее из этого множества случайно формируются пары моделей, которые "скрещиваются": формируется новое множество признаков, в котором каждый признак присутствует, если он есть в обеих моделях-родителях, не присутствует, если его нет в обеих моделях-родителях, и присутствует с вероятностью $\frac{1}{2}$, если он есть ровно в одной модели-родителе. Также на каждом шаге могут происходить мутации: случайное изменение множества признаков некоторых полученных моделей. Алгоритм продолжает работу, пока не выполняется критерий останова. Выбор критерия, а также другие детали, зависят от реализации.

\section{Анализ ошибки}

В данной работе особое внимание уделяется исследованию не только точности, но и устойчивости алгоритма. Для этого применяются различные критерии качества.

Неустойчивостью алгоритма будем называть статистически значимое изменение ошибки при незначимых изменениях выборки или  множества признаков.

Для того, чтобы исследовать неустойчивость изучаемых алгоритмов, предлагается некоторым образом немного изменить выборку и сравнить результат работы алгоритма. Чем больше неустойчивость алгоритма, тем больше будут отличаться результаты.

Введем некоторые обозначения.

Пусть m - количество наблюдений,

n - количество признаков,

q - размерность ответа,

$X \in R^{m \times n}$ - матрица признаков,

$y \in R^{m \times q}$ - истинные ответы,

$A \in \{0,1\}^n$ - булевая матрица отобранных признаков($A_i = 1 <=> i-й$ признак выбран),

$\theta \in R^{n \times q}$ - предсказанные линейной моделью коэффициенты,

$\hat{y}  = X\theta \in R^{m \times k}$ - предсказания линейной модели (построенной только с учетом отобранных признаков)

$S(y, \hat{y})$ -  функция ошибки (метрика качества). Мы считаем, что чем меньше ее значение, тем больше точность алгоритма.

$\epsilon = y - \hat{y}$ - ошибка предсказания

$l \in \{0, 1, 2, 3, 4\}$ - номер исследуемого алгоритма

$L$ - множество алгоритмов

Для того, чтобы вызвать незначительные изменения выборки, используется процедура бутстрепа. Мы будем создавать новые выборки, выбирая c возвращением m элементов исходной выборки. Фактически, таким образом будут генерироваться выборки из эмпирического распределения исходных данных.

Пусть k - количество бутстрепных выборок. K - множество бутсрепных выборок.

Для изменения множества параметров используется метод семплирования пространства параметров. Выбираются случайные подвыборки размера r < n. ???



Для каждой пары из $L \times K$ считается ошибка линейной модели при применении данного алгоритма отбора признаков. Чем больше ошибки разнятся от выборки к выборке, тем менее устойчив метод отбора.


Далее, наверное, надо конкретно уже описать.

\section{Метрики}

Характеристикой является сложность

Пусть имеются истинный прогноз $\bY = (\by_1, \by_2, \dots, \by_m)$ и предсказание $\mathbf{\hat{Y}} = (\mathbf{\hat{y}}_1, \mathbf{\hat{y}}_2, \dots, \mathbf{\hat{y}}_m)$; $\by_i$ и $\mathbf{\hat{y}}_i$, $i = 1, 2, \dots, m$ -- вектора размерности $r$.

Среднеквадратичная  ошибка (mean squared error):

\begin{equation}
	\text{MSE}(\bY, \mathbf{\hat{Y}}) =
	\frac{1}{m}
	\sum^m_{i=1}
	{\bigl\| \mathbf{y}_i - \mathbf{\hat{y}}_i \bigr\| }_2^2 
	\label{eq::error_mse}
\end{equation}

Корень среднеквадратичной ошибки (root-mean-squared error):

\begin{equation}
\text{RMSE}(\bY, \mathbf{\hat{Y}}) =
\sqrt{\text{MSE}(\bY, \mathbf{\hat{Y}})}
\label{eq::error_rmse}
\end{equation}

Нормированная среднеквадратичная ошибка (scaled mean squared error):

\begin{equation}
\text{sMSE}(\bY, \mathbf{\hat{Y}}) =
\frac{
\sum^m_{i=1}
{\bigl\| \mathbf{y}_i - \mathbf{\hat{y}}_i \bigr\| }_2^2 
}{
\sum^m_{i=1}
{\bigl\| \mathbf{y}_i - \mathbf{\overline{y}} \bigr\| }_2^2 
}, \;
\mathbf{\overline{y}} = \frac{1}{m} \sum_{i=1}^{m} y_i
\label{eq::error_smse}
\end{equation}

\paragraph{Мультиколлинеарность}
Предположим, что приз


Число обусловленности матрицы $\bX\T
\bX$ 


\begin{equation}
\kappa =  \frac{\lambda_{\textrm{max}}}{\lambda_{\textrm{min}}}
\end{equation}

Рассматриваются числа обусловленности для $\bX\T \bX$, $\widehat{\bY}\T \widehat{\bY}$. В случае применения модели PLS, будут рассматриваться также числа обусловленности $\bT\T \bT$


MAE, MADE, Коэффициент корреляции. Различные параметры модели из \cite{Katrutsa17QPFS}.

Сложность модели


\begin{thebibliography}{1}
\bibitem{bci}
    \BibAuthor{
    	J. del R.\; Mill?n,
    	F. \; Renken,
    	J. \; Mouri?o and
    	W. \; Gerstner}
    \BibTitle{Brain-actuated interaction}~//
    \BibJournal{Artif. Intell.}, 159(2004) 241–259.
\bibitem{Isachenko17PLS}
    \BibAuthor{
    	Isachenko \; R.,
    	Vladimirova \; M.,
    	Strijov \; V.}
    \BibTitle{Dimensionality reduction for time series decoding and
    	forecasting problems}~//
    \BibJournal{Machine Learning and Data Analysis}.
\bibitem{Katrutsa15StressTest}
	\BibAuthor{
	Katrutsa \; A.,
	Strijov \; V.}
	\BibTitle{Stress test procedure for feature selection algorithms}, 2015~//
\BibJournal{Expert System with Applications}, 142, 172–183
\bibitem{Katrutsa17QPFS}
    \BibAuthor{
    	Katrutsa \; A.,
    	Strijov \; V.}
    \BibTitle{Comprehensive study of feature selection methods to solve
    	multicollinearity problem according to evaluation criteria}, 2017~//
    \BibJournal{Expert System with Applications}, 76, 1-11.
\bibitem{Motrenko17ECoG}
	\BibAuthor{
		Motrenko \; A.,
		Strijov \; V.}
	\BibTitle{Multi-way Feature Selection for ECoG-based Brain-Computer
	Interface}~//
	\BibJournal{???}.

\end{thebibliography}

% Решение Программного Комитета:
%\ACCEPTNOTE
%\AMENDNOTE
%\REJECTNOTE
\end{document}
